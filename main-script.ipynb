{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning on Inner speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/home/lucsi/.pyenv/versions/3.10.6/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home/lucsi/.pyenv/versions/3.10.6/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#@title Install dependencies \n",
    "!git clone https://github.com/N-Nieto/Inner_Speech_Dataset -q\n",
    "!pip3 install mne -q\n",
    "!pip install umap-learn\n",
    "\n",
    "!pip3 install pyriemann\n",
    "from sklearn.cluster import KMeans\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.utils.base import invsqrtm\n",
    "!pip3 install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports \n",
    "import os\n",
    "import mne \n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import warnings\n",
    "import torchvision\n",
    "import gc\n",
    "import collections\n",
    "import umap\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from google.colab import drive, files\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "from Inner_Speech_Dataset.Python_Processing.Data_extractions import  Extract_data_from_subject, Extract_block_data_from_subject, Extract_data_multisubject\n",
    "from Inner_Speech_Dataset.Python_Processing.Data_processing import *\n",
    "\n",
    "np.random.seed(23)\n",
    "\n",
    "mne.set_log_level(verbose='warning') #to avoid info at terminal\n",
    "warnings.filterwarnings(action = \"ignore\", category = DeprecationWarning ) \n",
    "warnings.filterwarnings(action = \"ignore\", category = FutureWarning ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Set random seed\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)\n",
    "SEED = 23\n",
    "set_seed(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU).\n",
    "\n",
    "# inform the user if the notebook uses GPU or CPU.\n",
    "def set_device():\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"No GPU enabled in this notebook.\")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device\n",
    "\n",
    "\n",
    "DEVICE = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title función buscar path, sino crearlo {form-width:\"25%\"}\n",
    "\n",
    "def ensure_dir(dir_name):\n",
    "  import os\n",
    "  if not os.path.exists(dir_name):\n",
    "      os.makedirs(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Mount drive {form-width:\"25%\"}\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title extract_data_ms { form-width: \"25%\" }\n",
    "\n",
    "def Extract_data_multisubject_label(root_dir, N_S_list, datatype='EEG'):\n",
    "    \"\"\"\n",
    "    Load all blocks for a list of subject and stack the results in X\n",
    "    \"\"\"\n",
    "    print(\"Ejecutando Extract_data_multisubject_label\")\n",
    "\n",
    "    N_B_arr = [1, 2, 3]\n",
    "    tmp_list_X = []\n",
    "    tmp_list_Y = []\n",
    "    rows = []\n",
    "    total_elem = len(N_S_list)*3  # assume 3 sessions per subject\n",
    "    S = 0\n",
    "    for N_S in N_S_list:\n",
    "        print(\"Iteration \", S)\n",
    "        print(\"Subject \", N_S)\n",
    "        for N_B in N_B_arr:\n",
    "\n",
    "            # name correction if N_Subj is less than 10\n",
    "            if N_S < 10:\n",
    "                Num_s = 'sub-0'+str(N_S)\n",
    "            else:\n",
    "                Num_s = 'sub-'+str(N_S)\n",
    "\n",
    "            base_file_name = root_dir + '/derivatives/' + Num_s + \\\n",
    "                '/ses-0' + str(N_B) + '/' + Num_s+'_ses-0'+str(N_B)\n",
    "            events_file_name = base_file_name+'_events.dat'\n",
    "\n",
    "            if datatype == \"EEG\" or datatype == \"eeg\":\n",
    "                #  load data and events\n",
    "                eeg_file_name = base_file_name+'_eeg-epo.fif'\n",
    "                print(\"Inner iteration \", N_B)\n",
    "                data_tmp_X = mne.read_epochs(\n",
    "                    eeg_file_name, verbose='WARNING')._data\n",
    "                rows.append(data_tmp_X.shape[0])\n",
    "                if S == 0 and N_B == 1:  # assume same number of channels, time steps, and column labels in every subject and session\n",
    "                    print(\"saving channels and steps\")\n",
    "                    chann = data_tmp_X.shape[1]\n",
    "                    steps = data_tmp_X.shape[2]\n",
    "                    # columns=data_tmp_Y.shape[1]\n",
    "                tmp_list_X.append(data_tmp_X)\n",
    "                tmp_list_Y.append(np.ones(data_tmp_X.shape[0])*S)\n",
    "\n",
    "            # ToDo improvement not applied to exg and baseline datatypes yet\n",
    "            elif datatype == \"EXG\" or datatype == \"exg\":\n",
    "                file_name = root_dir + '/derivatives/' + Num_s + '/ses-0' + \\\n",
    "                    str(N_B) + '/' + Num_s+'_ses-0'+str(N_B)+'_exg-epo.fif'\n",
    "                X = mne.read_epochs(file_name, verbose='WARNING')\n",
    "                data[N_B] = X._data\n",
    "\n",
    "            elif datatype == \"Baseline\" or datatype == \"baseline\":\n",
    "                file_name = root_dir + '/derivatives/' + Num_s + '/ses-0' + \\\n",
    "                    str(N_B) + '/' + Num_s+'_ses-0' + \\\n",
    "                    str(N_B)+'_baseline-epo.fif'\n",
    "                X = mne.read_epochs(file_name, verbose='WARNING')\n",
    "                data[N_B] = X._data\n",
    "\n",
    "            else:\n",
    "                print(\"Invalid Datatype\")\n",
    "        S += 1\n",
    "\n",
    "    X = np.empty((sum(rows), chann, steps))\n",
    "    Y = np.empty((sum(rows)), dtype=np.int8)\n",
    "    offset = 0\n",
    "    # put elements of list into numpy array\n",
    "    for i in range(total_elem):\n",
    "        print(\"Saving element into array: \", i)\n",
    "        X[offset:offset+rows[i], :, :] = tmp_list_X[0]\n",
    "        Y[offset:offset+rows[i]] = tmp_list_Y[0]\n",
    "        offset += rows[i]\n",
    "        del tmp_list_X[0]\n",
    "        del tmp_list_Y[0]\n",
    "        gc.collect()\n",
    "    print(\"X shape\", X.shape)\n",
    "    print(\"Y shape\", Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def extract_data_ms(root_dir, datatype, subjects=None, real_task=True):\n",
    "    \"\"\"Dependiendo si se quiere pre entrenar un modelo con dummy task o real task,\n",
    "    se ejecutará el Extract_data implementado en github o el implementado localmente, el \n",
    "    cual reacomoda los labels\n",
    "\n",
    "    Args:\n",
    "        root_dir (_type_): path de archivos\n",
    "        datatype (_type_): tipo de archivo\n",
    "        real_task (bool, optional): real task o dummy task. Defaults to True.\n",
    "        subjects (list[integer], optional): lista de sujetos. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        X_raw (numpy array): Trials por sujeto\n",
    "        Y (numpy array): Labels\n",
    "    \"\"\"\n",
    "    print(\"Ejecutando extract_data_ms\")\n",
    "  # a subject is kept away for fine tuning\n",
    "    if (subjects == None):\n",
    "        L = [i+1 for i in range(10)]\n",
    "        L.pop(sub-1)\n",
    "\n",
    "        # # delete \"bad\" subjects from pre training\n",
    "        # L.pop(2-1)\n",
    "        # L.pop(4-1-1)\n",
    "        # L.pop(8-1-1-1)\n",
    "    else:\n",
    "        L = [subjects]\n",
    "    print(L)\n",
    "\n",
    "    # real task\n",
    "    if(real_task):\n",
    "        X_raw, Y = Extract_data_multisubject(root_dir, L, datatype=datatype)\n",
    "    else:\n",
    "        # dummy task\n",
    "        X_raw, Y = Extract_data_multisubject_label(\n",
    "            root_dir, L, datatype=datatype)\n",
    "\n",
    "    print(\"\\n \\n\")\n",
    "    print(X_raw.shape)\n",
    "    print(Y.shape)\n",
    "    print(Y.dtype)\n",
    "\n",
    "    return X_raw, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title select_time_window {form-width: \"25%\"}\n",
    "def cut_useful_time(X_raw, t_start=1.5, t_end=3.5, fs=256, float_point=None):\n",
    "    print(\"Ejecutando cut_useful_time\")\n",
    "    \"\"\"Se recortan los datos en la ventana de tiempo con mayor actividad (tiempo útil)\n",
    "\n",
    "    Args:\n",
    "        X_raw (_type_): Datos en crudo, retornados del extract multi subject\n",
    "        t_start (float, optional): inicio de ventana de tiempo. Defaults to 1.5.\n",
    "        t_end (float, optional): finalización de ventana de tiempo. Defaults to 3.5.\n",
    "        fs (int, optional): frecuencia de muestreo. Defaults to 256.\n",
    "        float_point (_type_, optional): bits para punto flotante. Defaults to None = 64bits.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # Cut useful time\n",
    "    import gc\n",
    "    X = Select_time_window(X=X_raw, t_start=t_start, t_end=t_end, fs=fs)\n",
    "    print(X.shape)\n",
    "    del X_raw\n",
    "    gc.collect()\n",
    "\n",
    "    X = cast_float_point(X, float_point)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def cast_float_point(X, float_point=None):\n",
    "    \"\"\"casteo de np array a los bits seleccionados\n",
    "\n",
    "    Args:\n",
    "        X (_type_): Ventana del tiempo útil de los datos. Datos retornado de Select_time_window\n",
    "        float_point (_type_, optional): bits para punto flotante. Defaults to None = 64bits.\n",
    "\n",
    "    Returns:\n",
    "        _type_: numpy array\n",
    "    \"\"\"\n",
    "    if float_point == 32:\n",
    "        X = np.float32(X)\n",
    "    elif float_point == 16:\n",
    "        X = np.float16(X)\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title data_processing {form-width: \"25%\"}\n",
    "def transform_for_classificator(X, Y, Classes, Conditions, real_task=True):\n",
    "    print(\"Ejecutando transform_for_classificator\")\n",
    "    if(real_task):\n",
    "        # for pre train with real task\n",
    "        X, Y = Transform_for_classificator(X, Y, Classes, Conditions)\n",
    "    else:\n",
    "        # for pre train with dummy task\n",
    "        X, Y = Transform_for_classificator_label(X, Y, Classes, Conditions)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def split_data(X, Y, test_size, val_size, scl, SEED, window_len, window_step, fs=256, pre_train=False):\n",
    "    print(\"Ejecutando split_data\")\n",
    "    if(pre_train):\n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "            X, Y, test_size=val_size, random_state=SEED, stratify=Y)\n",
    "        # Split Trial\n",
    "        del X, Y\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        X_train, Y_train = Split_trial_in_time(\n",
    "            X_train, Y_train, window_len, window_step, fs)\n",
    "        X_val, Y_val = Split_trial_in_time(\n",
    "            X_val, Y_val, window_len, window_step, fs)\n",
    "        X_test = None\n",
    "        Y_test = None\n",
    "        len_window_sample = None\n",
    "    else:\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            X, Y, test_size=test_size, random_state=SEED, stratify=Y)\n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "            X_train, Y_train, test_size=val_size, random_state=SEED, stratify=Y_train)\n",
    "        X_in_shape = X_train.shape[0]\n",
    "\n",
    "        X_train, Y_train = Split_trial_in_time(\n",
    "            X_train, Y_train, window_len, window_step, fs)\n",
    "        X_val, Y_val = Split_trial_in_time(\n",
    "            X_val, Y_val, window_len, window_step, fs)\n",
    "        X_test, Y_test = Split_trial_in_time(\n",
    "            X_test, Y_test, window_len, window_step, fs)\n",
    "        X_out_shape = X_train.shape[0]\n",
    "        len_window_sample = int(X_out_shape/X_in_shape)\n",
    "\n",
    "    # Scale to [0,1] each channel\n",
    "    for chn in range(X_train.shape[1]):\n",
    "        X_train[:, chn, :] = scl.fit_transform(X_train[:, chn, :])\n",
    "\n",
    "        X_val[:, chn, :] = scl.transform(X_val[:, chn, :])\n",
    "        if(not pre_train):\n",
    "            X_test[:, chn, :] = scl.transform(X_test[:, chn, :])\n",
    "\n",
    "    # print(\"Final data shape\")\n",
    "    # print(\"X_train.shape\", X_train.shape)\n",
    "    # print(\"X_val.shape\", X_val.shape)\n",
    "    # print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "    # print(\"Data atributes\")\n",
    "    # print(\"Min:\" + str(X_train.min()))\n",
    "    # print(\"Max:\" + str(X_train.max()))\n",
    "    # print(\"Data Type:\" + str(X_train.dtype))\n",
    "\n",
    "    return X_train, X_val, X_test, Y_train, Y_val, Y_test, len_window_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title euclidean_aligment {form-width:\"25%\"}\n",
    "\n",
    "\n",
    "\n",
    "def EU(Xtr, Xval, Xte=None):\n",
    "    print(\"Ejecutando EU\")\n",
    "    \"\"\"Euclidean Aligment\n",
    "\n",
    "    Args:\n",
    "        Xtr (_type_): los sujetos del pretrn\n",
    "        Xval (_type_): el sujeto a finetunear\n",
    "        Xte (_type_, optional): el sujeto a finetunear. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        _type_: \n",
    "        Xtr_eu -> los datos de los sujetos utilizados en pre-train, alineados (para luego subdividir en train y validacion)\n",
    "        Xval_eu -> los datos del sujeto utilizado en fine tuning, alineados, para subdividir en train y validacion del fine tuning\n",
    "        Xte_eu -> los datos del sujeto utilizado en fine tuning, alineados, para test\n",
    "    \"\"\"\n",
    "\n",
    "    # Estimate single trial covariance\n",
    "    cov_tr = Covariances().transform(Xtr)\n",
    "    # cov_tr_fine= Covariances().transform(Xtr_fine)\n",
    "\n",
    "    Ctr = cov_tr.mean(0)\n",
    "    # Ctr_fine = cov_tr_fine.mean(0)\n",
    "\n",
    "    # aligment\n",
    "    Xtr_eu = np.asarray([np.dot(invsqrtm(Ctr), epoch) for epoch in Xtr])\n",
    "    Xval_eu = np.asarray([np.dot(invsqrtm(Ctr), epoch) for epoch in Xval])\n",
    "    if (Xte is not None):\n",
    "        Xte_eu = np.asarray([np.dot(invsqrtm(Ctr), epoch) for epoch in Xte])\n",
    "    else:\n",
    "        Xte_eu = None\n",
    "\n",
    "    return Xtr_eu, Xval_eu, Xte_eu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title cast_to_dataset {form-width:\"25%\"}\n",
    "\n",
    "def convert_to_dataset(X_numpy, Y_numpy, DEVICE):\n",
    "  print(\"Ejecutando convert_to_dataset\")\n",
    "  # Convert data from numpy to Tensor\n",
    "  tensor_X = torch.Tensor(X_numpy).to(DEVICE)\n",
    "  import gc\n",
    "  del X_numpy\n",
    "  gc.collect()\n",
    "  # Add the \"channel\" dimension for CNNs\n",
    "  tensor_X = torch.unsqueeze(tensor_X,1).to(DEVICE)\n",
    "\n",
    "  # convert labels to tensor\n",
    "  tensor_Y = torch.Tensor(Y_numpy).to(DEVICE)\n",
    "  import gc\n",
    "  del Y_numpy\n",
    "  gc.collect()\n",
    "  tensor_Y = tensor_Y.type(torch.LongTensor)\n",
    "\n",
    "  # Create a Pytorch Dataset\n",
    "  Dataset_tensor = torch.utils.data.TensorDataset(tensor_X,tensor_Y)\n",
    "  import gc\n",
    "  del tensor_X, tensor_Y\n",
    "  gc.collect()\n",
    "  return Dataset_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Inet {form-width:\"25%\"}\n",
    "\n",
    "class INet(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(INet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 4, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(8, 16, 3, padding=1)\n",
    "\n",
    "        self.drop1 = nn.Dropout(p=0.1)\n",
    "        self.drop2 = nn.Dropout(p=0.2)\n",
    "        self.drop3 = nn.Dropout(p=0.3)\n",
    "#(1280x256 and 1536x8)\n",
    "        self.fc1 = nn.Linear(256, 8)  # for 0.2 len\n",
    "        self.fc2 = nn.Linear(8, n_classes)\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title modelo {form-width: \"25%\"}\n",
    "from torchinfo import summary\n",
    "def init_net(Y, pre_train, save_dir, DEVICE):\n",
    "    print(\"Ejecutando init_net\")\n",
    "    n_classes = len(np.unique(Y))\n",
    "    print(\"Total classes: \", n_classes)\n",
    "    in_net = INet(n_classes=n_classes).to(DEVICE)\n",
    "    print(\"Total Parameters in Network {:10d}\".format(\n",
    "        sum(p.numel() for p in in_net.parameters())))\n",
    "    # uncomment line below to get model summary\n",
    "    # print(summary(in_net, (128, 1, 128, 51)))\n",
    "\n",
    "    if pre_train:\n",
    "        print(\"loading model\")\n",
    "        in_net.load_state_dict(torch.load(save_dir+'/best_model.pt'))\n",
    "        in_net.fc2 = nn.Linear(8, n_classes).to(\n",
    "            DEVICE)  # fine tune the last linear layer\n",
    "    return in_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title test_model {form-width: \"25%\"}\n",
    "\n",
    "def Most_Common(lst):\n",
    "    # https://stackoverflow.com/questions/1518522/find-the-most-common-element-in-a-list\n",
    "    import collections\n",
    "    # print(lst)\n",
    "    data = collections.Counter(lst)\n",
    "    return data.most_common(1)[0][0], data\n",
    "\n",
    "\n",
    "def test_model(best_model, test_loader, DEVICE, len_window_sample):\n",
    "    print(\"Ejecutando test_model\")\n",
    "    # Total data\n",
    "    total_vot = 0\n",
    "    total = 0\n",
    "    # Correct data\n",
    "    correct_vot = 0\n",
    "    correct = 0\n",
    "\n",
    "    #p = 0\n",
    "    predicted_err = []  # Y PREDICHO\n",
    "    target_err = []  # Y TEST\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        # Get data in Device\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        # Predict with the best model the data\n",
    "        output = best_model(data)\n",
    "\n",
    "        # get accuracy\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        acc = correct/total\n",
    "\n",
    "        # get accuracy (votacion)\n",
    "        steps = int(output.shape[0]/len_window_sample)\n",
    "        print(\"steps\", steps)\n",
    "        for i in range(0, steps):\n",
    "            _, predicted_vot = torch.max(\n",
    "                output[len_window_sample*i:len_window_sample*(i+1), :], 1)\n",
    "\n",
    "            predicted_vot, data_x = Most_Common(predicted_vot)\n",
    "            target_a, data_y = Most_Common(\n",
    "                target[len_window_sample*i:len_window_sample*(i+1)])\n",
    "\n",
    "            # comparar cuantos le pega por trial\n",
    "            aux_x = sorted(data_x.elements())\n",
    "            aux_y = sorted(data_y.elements())\n",
    "            # Si predicho y real difieren, los guardo para sacar estadisticos\n",
    "            if(aux_x != aux_y):\n",
    "                predicted_err.append(aux_x)\n",
    "                target_err.append(aux_y)\n",
    "\n",
    "            #p += 1\n",
    "            total_vot += 1\n",
    "            correct_vot += (predicted_vot == target_a).sum().item()\n",
    "\n",
    "    acc_vot = correct_vot/total_vot\n",
    "    print(\"Final Test Acc\")\n",
    "    print(acc_vot)\n",
    "    print(\"trials dif\", len(predicted_err))\n",
    "    #accuracy normal, acc con votacion, trials en los que no se acerto en al menos una ventana\n",
    "    return acc, acc_vot, predicted_err, target_err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title train_loop {form-width: \"25%\"}\n",
    "\n",
    "# v5, use early stopping for both pre training and fine tuning, pre training with real task\n",
    "def train(model, device, train_loader, validation_loader, epochs, criterion, optimizer, epochs_er_stop, min_epochs, plotting=True, plot_type=\"loss\"):\n",
    "    print(\"Ejecutando train\")\n",
    "    from IPython import display\n",
    "    validation_acc_ref = 0\n",
    "    loss_ref = 3\n",
    "    tol = 0.001\n",
    "    count = 0\n",
    "    train_loss, validation_loss = [], []\n",
    "    train_acc, validation_acc = [], []\n",
    "    # to run GUI event loop\n",
    "    plt.ion()\n",
    "    with tqdm(range(epochs), unit='epoch') as tepochs:\n",
    "        tepochs.set_description('Training')\n",
    "        for epoch in tepochs:\n",
    "            print(epoch)\n",
    "            model.train()\n",
    "\n",
    "            # keeps track of the running loss\n",
    "            running_loss = 0.\n",
    "            correct, total = 0, 0\n",
    "            for data, target in train_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                # 1. Get the model output (call the model with the data from this batch)\n",
    "                output = model(data)\n",
    "\n",
    "                # 2. Zero the gradients out (i.e. reset the gradient that the optimizer\n",
    "                #                       has collected so far with optimizer.zero_grad())\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 3. Get the Loss (call the loss criterion with the model's output\n",
    "                #                  and the target values)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                # 4. Calculate the gradients (do the pass backwards from the loss\n",
    "                #                             with loss.backward())\n",
    "                loss.backward()\n",
    "\n",
    "                # 5. Update the weights (using the training step of the optimizer,\n",
    "                #                        optimizer.step())\n",
    "                optimizer.step()\n",
    "\n",
    "                # set loss to whatever you end up naming your variable when\n",
    "                # calling criterion\n",
    "                # for example, loss = criterion(output, target)\n",
    "                # then set loss = loss.item() in the set_postfix function\n",
    "                tepochs.set_postfix(loss=loss.item())\n",
    "                running_loss += loss.item()  # add the loss for this batch\n",
    "\n",
    "                # get accuracy\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "\n",
    "            # append the loss for this epoch (running loss divided by the number of batches e.g. len(train_loader))\n",
    "            train_loss.append(running_loss/len(train_loader))\n",
    "            train_acc.append(correct/total)\n",
    "\n",
    "            # evaluate on validation data\n",
    "            model.eval()\n",
    "            running_loss = 0.\n",
    "            correct, total = 0, 0\n",
    "            for data, target in validation_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                tepochs.set_postfix(loss=loss.item())\n",
    "                running_loss += loss.item()\n",
    "                # get accuracy\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "            print(\"epoch {} running loss {}\".format(\n",
    "                epoch, running_loss/len(validation_loader)))\n",
    "            print(\"loss ref \", loss_ref)\n",
    "            # Early stoping\n",
    "            if running_loss/len(validation_loader) > loss_ref - tol:\n",
    "                print(\"detected condition for early stopping\")\n",
    "                count += 1\n",
    "                # Number of epochs with not improving loss\n",
    "                print(\"count is {}\".format(count))\n",
    "                if count == epochs_er_stop:\n",
    "                    if epoch > min_epochs:\n",
    "                        print(\"breaking training loop\")\n",
    "                        count = 0\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"reached early stopping condition, but no more than {} epochs have passed\".format(\n",
    "                            min_epochs))\n",
    "            else:\n",
    "                print(\"loss was better than before, count 0\")\n",
    "                # Restart count\n",
    "                # Update the reference loss\n",
    "                loss_ref = running_loss/len(validation_loader)\n",
    "                print(\"loss ref is now {}\".format(loss_ref))\n",
    "                count = 0\n",
    "\n",
    "            validation_loss.append(running_loss/len(validation_loader))\n",
    "            validation_acc.append(correct/total)\n",
    "            print(\"val accuracy for this batch is \", correct/total)\n",
    "            # loss_ref=running_loss/len(validation_loader)\n",
    "            if correct/total > validation_acc_ref:\n",
    "                print(\"Save best model in epoche\" + str(epoch) +\n",
    "                      \"with validation ACC : \" + str(correct/total))\n",
    "                best_model = copy.deepcopy(model)\n",
    "                best_epoch = epoch\n",
    "                validation_acc_ref = correct/total\n",
    "\n",
    "            # Loss eval\n",
    "            if plotting:\n",
    "                print(\"im plotting\")\n",
    "                plt.title('Train vs Valid Loss')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.legend(['Train', 'Valid'])\n",
    "                plt.xlabel('epoch')\n",
    "                # plt.clf()\n",
    "                if plot_type == \"loss\":\n",
    "                    plt.title('Train vs Valid Loss')\n",
    "                    plt.ylabel('Loss')\n",
    "                    plt.legend(['Train', 'Valid'])\n",
    "                    plt.xlabel('epoch')\n",
    "                    plt.plot(train_loss)\n",
    "                    plt.plot(validation_loss)\n",
    "                    # to refresh same plot on every loop\n",
    "                    # time.sleep(1)\n",
    "                    display.display(plt.gcf())\n",
    "                    display.clear_output(wait=True)\n",
    "                    plt.show()\n",
    "\n",
    "                # plt.show()\n",
    "                # flag = 0\n",
    "\n",
    "    return train_loss, train_acc, validation_loss, validation_acc, best_model, best_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title trainer {form-width:\"25%\"}\n",
    "\n",
    "def trainer(in_net, train_loader, val_loader, lr, epochs, min_epochs, epochs_er_stop, DEVICE, save_dir, sub, plotting, pre_train):\n",
    "    print(\"Ejecutando trainer\")\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, in_net.parameters()), lr=lr)\n",
    "\n",
    "    # Loss\n",
    "    criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "    # Train model\n",
    "    train_loss, train_acc, validation_loss, validation_acc, best_model, best_epoch = train(in_net, DEVICE, train_loader, val_loader,\n",
    "                                                                                           epochs, criterion, optimizer, epochs_er_stop=epochs_er_stop,\n",
    "                                                                                           min_epochs=min_epochs, plotting=plotting)\n",
    "\n",
    "    if(pre_train):\n",
    "      save_dir = save_dir+'/pre_train/subj-'+str(sub)\n",
    "      ensure_dir(save_dir+'/best_model.pt')\n",
    "      torch.save(best_model.state_dict(), save_dir+'/best_model.pt')\n",
    "      print(\"saved best model in epoch \", best_epoch)\n",
    "\n",
    "    return train_loss, train_acc, validation_loss, validation_acc, best_model, best_epoch\n",
    "\n",
    "\n",
    "def trainer_kfold(X, Y, eu_aligment, k_fold, batch_size, lr, epochs, min_epochs, epochs_er_stop, DEVICE, save_dir, sub, plotting):\n",
    "    print(\"Ejecutando trainer_kfold\")\n",
    "    accs_vote = []\n",
    "    accs_normal = []\n",
    "  \n",
    "    save_loss_dir = save_dir+'/results/loss'\n",
    "    ensure_dir(save_loss_dir)\n",
    "    save_acc_dir = save_dir+'/results/acc'\n",
    "    ensure_dir(save_acc_dir)\n",
    "\n",
    "    for i in range(k_fold):\n",
    "        ################################\n",
    "        # split para fine-tuning\n",
    "        X_train, X_val, X_test, Y_train, Y_val, Y_test, len_window_sample = split_data(\n",
    "            X, Y, test_size=test_len, val_size=val_len, scl=scl, SEED=i, window_len=window_len, window_step=window_step, fs=fs, pre_train=False)\n",
    "\n",
    "        ################################\n",
    "        # alineamiento euclideo para fine-tuning\n",
    "        if(eu_aligment):\n",
    "            X_train, X_val, X_test = EU(Xtr=X_train, Xval=X_val, Xte=X_test)\n",
    "\n",
    "        Dataset_tensor_train = convert_to_dataset(X_train, Y_train, DEVICE)\n",
    "        Dataset_tensor_val = convert_to_dataset(X_val, Y_val, DEVICE)\n",
    "        Dataset_tensor_test = convert_to_dataset(X_test, Y_test, DEVICE)\n",
    "\n",
    "        #Dataloaders\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            Dataset_tensor_train, batch_size=batch_size, shuffle=True)  # probar con shuffle en true\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            Dataset_tensor_val, batch_size=batch_size, shuffle=True)  # probar con shuffle en true\n",
    "        test_loader = torch.utils.data.DataLoader(Dataset_tensor_test, batch_size=int(\n",
    "            len_window_sample*batch_size), shuffle=False)\n",
    "        \n",
    "        # Definición de parámetros\n",
    "        in_net = init_net(Y_train, pre_train, save_dir, DEVICE)\n",
    "\n",
    "        train_loss, train_acc, validation_loss, validation_acc, best_model, best_epoch = trainer(\n",
    "            in_net, train_loader, val_loader, lr, epochs, min_epochs, epochs_er_stop, DEVICE, save_dir, sub, plotting=plotting, pre_train=False)\n",
    "\n",
    "        print(\"saved best model in epoch \", best_epoch)\n",
    "        # Plot final results\n",
    "        plt.figure()\n",
    "        plt.plot(train_loss)\n",
    "        plt.plot(validation_loss)\n",
    "        plt.title(\"Train and validation loss for fold \" + str(i))\n",
    "        plt.grid()\n",
    "        \n",
    "        plt.savefig(save_loss_dir+'/fold_'+str(i)+'-loss.png')\n",
    "        \n",
    "        # Plot final results\n",
    "        plt.figure()\n",
    "        plt.plot(train_acc)\n",
    "        plt.plot(validation_acc)\n",
    "        plt.title(\"Train and validation accuracy for fold \" + str(i))\n",
    "        plt.grid()\n",
    "        \n",
    "        plt.savefig(save_acc_dir+'/fold_'+str(i)+'-acc.png')\n",
    "\n",
    "        acc_normal, acc_vote, predicted_err, target_err = test_model(\n",
    "            best_model, test_loader, DEVICE, len_window_sample)\n",
    "        accs_vote.append(acc_vote)\n",
    "        accs_normal.append(acc_normal)\n",
    "        with open(save_acc_dir+'/accs-sub'+str(sub)+'.txt', 'w') as filehandle:\n",
    "            filehandle.writelines(\"%s\\n\" % a for a in accs_vote)\n",
    "\n",
    "    # print(accs)\n",
    "    return accs_vote, accs_normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title wrapper pre-train {form-width:\"25%\"}\n",
    "def wrapper_pre_train(root_dir, save_dir, datatype, t_start, t_end, fs, float_point, Classes, Conditions, batch_size_pre, val_len, window_len, window_step, scl, SEED, eu_aligment, DEVICE, sub, plotting):\n",
    "  print(\"Ejecutando PRE-TRAINING\")\n",
    "  X_raw, Y = extract_data_ms(root_dir, datatype)\n",
    "\n",
    "  import gc\n",
    "  #################################\n",
    "  # Selección de tiempo útil\n",
    "  X = cut_useful_time(X_raw, t_start=t_start, t_end=t_end,\n",
    "                      fs=fs, float_point=float_point)\n",
    "  del X_raw\n",
    "  gc.collect()\n",
    "  ################################\n",
    "  # data processing\n",
    "  # transform for classificator para pre-train\n",
    "  X, Y = transform_for_classificator(X, Y, Classes, Conditions)\n",
    "\n",
    "  ################################\n",
    "  # split para pre train\n",
    "  X_train, X_val, _, Y_train, Y_val, _, _ = split_data(\n",
    "      X, Y, test_size=None, val_size=val_len, scl=scl, SEED=SEED, window_len=window_len, window_step=window_step, fs=fs, pre_train=True)\n",
    "  del X, Y\n",
    "  gc.collect()\n",
    "\n",
    "  #################################\n",
    "  # Alineamiento euclideo para pre-train\n",
    "  if (eu_aligment):\n",
    "      X_train, X_val, _ = EU(Xtr=X_train, Xval=X_val, Xte=None)\n",
    "\n",
    "\n",
    "  #################################\n",
    "  # castear a dataset\n",
    "  dataset_tensor_train = convert_to_dataset(X_train, Y_train, DEVICE)\n",
    "  dataset_tensor_val = convert_to_dataset(X_val, Y_val, DEVICE)\n",
    "\n",
    "  train_loader_pre = torch.utils.data.DataLoader(\n",
    "      dataset_tensor_train, batch_size=batch_size_pre, shuffle=True)\n",
    "  val_loader_pre = torch.utils.data.DataLoader(\n",
    "      dataset_tensor_val, batch_size=batch_size_pre, shuffle=False)\n",
    "\n",
    "\n",
    "  ###############################\n",
    "  #   ENTRENAMIENTO PRE TRAIN   #\n",
    "  ###############################\n",
    "\n",
    "  # definicion de parametros\n",
    "  in_net = init_net(Y_train, False, save_dir, DEVICE)\n",
    "  lr = 0.001\n",
    "  epochs = 200\n",
    "  min_epochs = 40\n",
    "  epochs_er_stop = 20\n",
    "\n",
    "  train_loss, train_acc, validation_loss, validation_acc, best_model, best_epoch = trainer(\n",
    "      in_net, train_loader_pre, val_loader_pre, lr, epochs, min_epochs, epochs_er_stop, DEVICE, root_dir, sub, plotting, pre_train)\n",
    "  plt.plot(train_acc)\n",
    "  plt.plot(validation_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title wrapper_fine_tuning {form-width:\"25%\"}\n",
    "\n",
    "def wrapper_fine_tuning(root_dir, model_name, datatype, sub, t_start, t_end, fs, float_point, Classes, Conditions, test_len, val_len, window_len, window_step, scl, SEED, eu_aligment, DEVICE, plotting):\n",
    "    # Carga de los datos para los sujetos seleccionados, en el caso del fine tuning será uno solo\n",
    "    X_raw_fine, Y_fine = extract_data_ms(root_dir, datatype, subjects=sub)\n",
    "    # print(X_raw_fine.shape)\n",
    "    # print(Y_fine.shape)\n",
    "    # print(Y_fine.dtype)\n",
    "\n",
    "    #################################\n",
    "    # Seleccion de tiempo útil\n",
    "    X_fine = cut_useful_time(X_raw_fine, t_start=t_start,\n",
    "                             t_end=t_end, fs=fs, float_point=float_point)\n",
    "\n",
    "    ################################\n",
    "    # data processing\n",
    "    # transform for classificator para fine-tuning\n",
    "    X, Y = transform_for_classificator(X_fine, Y_fine, Classes, Conditions)\n",
    "\n",
    "    ##############################\n",
    "    # ENTRENAMIENTO FINE-TUNING  #\n",
    "    #############################\n",
    "\n",
    "    # Learning rate\n",
    "    lr = 0.001\n",
    "    epochs = 200\n",
    "    min_epochs = 40\n",
    "    # Max Epoches\n",
    "\n",
    "    # Epoches for Early stopping\n",
    "    epochs_er_stop = 100\n",
    "    # # Create a Pytorch Dataset\n",
    "    batch_size = 128\n",
    "\n",
    "    k_fold = 20\n",
    "\n",
    "    # K-fold\n",
    "    accs_normal, accs_vote = trainer_kfold(X, Y, eu_aligment, k_fold, batch_size, lr, epochs, min_epochs, epochs_er_stop, DEVICE, save_dir, sub, plotting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Parámetros globales {form-width:\"25%\"}\n",
    "\n",
    "#plot epoca a epoca\n",
    "plotting = False\n",
    "\n",
    "# Path de archivos (Raiz)\n",
    "root_dir = \"/gdrive/My Drive/DataSet\"\n",
    "\n",
    "#Seleccionar tipo de entrenamiento\n",
    "# True = pre train + fine tuning | False = entrenamiento simple\n",
    "pre_train = True\n",
    "\n",
    "# alineamiento euclideo\n",
    "eu_aligment = True\n",
    "\n",
    "#Armo el path dependiendo el entrenamiento\n",
    "if(pre_train and eu_aligment):\n",
    "  tipo_entrenamiento = '/pre_train_EU_005/'\n",
    "elif(eu_aligment):\n",
    "  tipo_entrenamiento = '/simple_EU/'\n",
    "else:\n",
    "  tipo_entrenamiento = '/simple/'\n",
    "# tipo_entrenamiento = '/pre_train/'\n",
    "\n",
    "\n",
    "#Sujeto a testear\n",
    "sub = 1\n",
    "save_dir = root_dir + tipo_entrenamiento + 'subj-' + str(sub)\n",
    "# path para guardar resultados\n",
    "ensure_dir(save_dir)\n",
    "\n",
    "datatype = \"EEG\"\n",
    "# change float point of data for faster implementations, default: float64\n",
    "float_point = 32\n",
    "\n",
    "# Sampling rate\n",
    "fs = 256\n",
    "\n",
    "# Time window analysis\n",
    "t_start = 1.5\n",
    "t_end = 3.5\n",
    "# Window's parameters to crop each trial\n",
    "window_len = 0.05\n",
    "window_step = 0.05\n",
    "\n",
    "# Test Examples %\n",
    "test_len = 0.2\n",
    "\n",
    "# Examples within train set used as validation %\n",
    "val_len = 0.2\n",
    "\n",
    "\n",
    "# Parámetros pre-train\n",
    "# Batch size\n",
    "# batch_size = 32\n",
    "batch_size_pre = 1280\n",
    "\n",
    "\n",
    "# Compared conditions\n",
    "Conditions = [[\"Inner\"], [\"Vis\"]]\n",
    "\n",
    "# Choos the classes to groop and comare\n",
    "Classes = [[\"All\"], [\"All\"]]\n",
    "\n",
    "# Scaler\n",
    "scl = MinMaxScaler()\n",
    "\n",
    "# Se selecciona que tarea realizará el pre entrenamiento (igual no se usa)\n",
    "# real_task = True -> tarea real\n",
    "# real_task = False -> tarea dummy\n",
    "real_task = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Definición main {form-width:\"25%\"}\n",
    "\n",
    "if(pre_train):\n",
    "  print(\"pre train\")\n",
    "  wrapper_pre_train(root_dir, save_dir, datatype, t_start, t_end, fs, float_point, Classes, Conditions,\n",
    "                  batch_size_pre, val_len, window_len, window_step, scl, SEED, eu_aligment, DEVICE, sub, plotting)\n",
    "  \n",
    "wrapper_fine_tuning(root_dir, save_dir, datatype, sub, t_start, t_end, fs, float_point, Classes,\n",
    "                   Conditions, test_len, val_len, window_len, window_step, scl, SEED, eu_aligment, DEVICE, plotting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfec9b63b5fb2bb16649bc822c11d4eee8ede932214af0107838ef86ab2373ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
